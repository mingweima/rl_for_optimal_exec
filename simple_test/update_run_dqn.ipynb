{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf             # Deep Learning library\n",
    "import numpy as np                  # Handle matrices\n",
    "import matplotlib.pyplot as plt     # Display graphs\n",
    "from collections import deque       # Ordered collection with ends\n",
    "import random\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "from simple_env import Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/gongqili/Desktop/sample_v1.csv'\n",
    "raw_data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = raw_data.drop(['#RIC', 'Domain', 'GMT Offset', 'Type', 'L1-BuyNo', 'L1-SellNo', 'L2-BuyNo', 'L2-SellNo',\n",
    "                      'L3-BuyNo', 'L3-SellNo', 'L4-BuyNo', 'L4-SellNo', 'L5-BuyNo', 'L5-SellNo',\n",
    "                      'L6-BuyNo', 'L6-SellNo', 'L7-BuyNo', 'L7-SellNo', 'L8-BuyNo', 'L8-SellNo',\n",
    "                      'L9-BuyNo', 'L9-SellNo', 'L10-BuyNo', 'L10-SellNo'], axis=1)\n",
    "data['Date-Time'] = pd.to_datetime(data['Date-Time'],\n",
    "                                   format='%Y-%m-%dT%H:%M:%S.%fZ').dt.round('{}s'.format(600))\n",
    "data = data.groupby(['Date-Time']).first().reset_index()\n",
    "data['Day'] = data['Date-Time'].dt.dayofweek\n",
    "data = data.drop(data.loc[(data['Day'] == 5) | (data['Day'] == 6)].index)\n",
    "data['Hour'] = data['Date-Time'].dt.hour\n",
    "data['Minute'] = data['Date-Time'].dt.minute\n",
    "data = data.drop(\n",
    "    data.loc[(data['Hour'] < 8) | (data['Hour'] > 16) | ((data['Hour'] == 16) & (data['Minute'] > 0))].index)\n",
    "data = data.drop(['Hour', 'Minute', 'Day'], axis=1)\n",
    "data = data.iloc[1:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Simulator(data)\n",
    "state = env.reset(num_days=0)\n",
    "state = np.array(state)\n",
    "#state = state.reshape(state.shape + (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_actions = [[1,0,0,0,0,0], [0,1,0,0,0,0], [0,0,1,0,0,0],\n",
    "                   [0,0,0,1,0,0], [0,0,0,0,1,0], [0,0,0,0,0,1]]\n",
    "# possible_actions = [[1,0], [0,1]]\n",
    "\n",
    "### MODEL HYPERPARAMETERS\n",
    "state_size = state.shape      # Our input is a stack of 4 frames hence 110x84x4 (Width, height, channels)\n",
    "action_size = len(possible_actions) # 8 possible actions\n",
    "initial_learning_rate =  0.00001     # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_loop = 2000\n",
    "total_episodes = 9\n",
    "max_steps = 5000              # Max possible steps in an episode\n",
    "batch_size = 64                # Batch size\n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.2            # minimum exploration probability\n",
    "decay_rate = 0.001           # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.5                    # Discounting rate\n",
    "max_tau = 500 #Tau is the C step where we update our target network\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 2000          # Number of experiences the Memory can keep\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "class DDDQNNet:\n",
    "    def __init__(self, state_size, action_size, initial_learning_rate, name):\n",
    "        # define learning rate\n",
    "        #         self.global_step = tf.Variable(0, trainable=False)\n",
    "        #         self.initial_learning_rate = initial_learning_rate  # 初始学习率\n",
    "        #         self.learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step=self.global_step, decay_steps=200, decay_rate=0.99, staircase=False)\n",
    "        #         self.add_global = self.global_step.assign_add(1)\n",
    "\n",
    "        # define input shapes\n",
    "        self.learning_rate = initial_learning_rate\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.name = name\n",
    "\n",
    "        # We use tf.variable_scope here to know which network we're using (DQN or target_net)\n",
    "        # it will be useful when we will update our w- parameters (by copy the DQN parameters)\n",
    "        with tf.variable_scope(self.name):\n",
    "            # We create the placeholders\n",
    "            # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\n",
    "            # [None, 100, 120, 4]\n",
    "            # Remember that target_Q is the R(s,a) + ymax Qhat(s', a')\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, action_size], name=\"actions_\")\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "\n",
    "            # define network\n",
    "            # Input is 100x120x4\n",
    "            self.conv_first1 = tf.keras.layers.LSTM(64, return_sequences=True)(self.inputs_)\n",
    "            self.conv_first1 = tf.keras.layers.LeakyReLU(alpha=0.01)(self.conv_first1)\n",
    "\n",
    "            ## Here we separate into two streams\n",
    "            # The one that calculate V(s)\n",
    "            self.value_fc = tf.keras.layers.LSTM(32)(self.conv_first1)\n",
    "            #             self.value_fc = tf.keras.layers.LeakyReLU(alpha=0.01)(self.value_fc)\n",
    "            self.value = tf.keras.layers.Dense(1)(self.value_fc)\n",
    "\n",
    "            #             The one that calculate A(s,a)\n",
    "            self.advantage_fc = tf.keras.layers.LSTM(32)(self.conv_first1)\n",
    "            #             self.advantage_fc = tf.keras.layers.LeakyReLU(alpha=0.01)(self.advantage_fc)\n",
    "            self.advantage = tf.keras.layers.Dense(self.action_size)(self.advantage_fc)\n",
    "\n",
    "            # Agregating layer\n",
    "            # Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a'))\n",
    "            self.output = self.value + tf.subtract(self.advantage,\n",
    "                                                   tf.reduce_mean(self.advantage, axis=1, keepdims=True))\n",
    "\n",
    "            self.output_softmax = tf.keras.activations.softmax(self.output)\n",
    "\n",
    "            # Q is our predicted Q value.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "\n",
    "            # The loss is modified because of PER\n",
    "            self.absolute_errors = tf.abs(self.target_Q - self.Q)  # for updating Sumtree\n",
    "\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.target_Q, self.Q))\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate, beta1=0.9, beta2=0.999, epsilon=1).minimize(\n",
    "                self.loss)\n",
    "            # self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate, epsilon=0.1).minimize(self.loss)\n",
    "\n",
    "            # record my values\n",
    "            self.display_action = tf.argmax(self.actions_, axis=1)\n",
    "\n",
    "    def record_tensorboard(self):\n",
    "        tf.summary.scalar(\"learning_rate\", self.learning_rate)\n",
    "        tf.summary.histogram(\"inputs_\", self.inputs_)\n",
    "        tf.summary.histogram(\"actions_\", self.display_action)\n",
    "        tf.summary.histogram(\"target_Q\", self.target_Q)\n",
    "        tf.summary.histogram(\"conv_first1\", self.conv_first1)\n",
    "        tf.summary.histogram(\"value_fc\", self.value_fc)\n",
    "        tf.summary.histogram(\"value\", self.value)\n",
    "        tf.summary.histogram(\"adv_fc\", self.advantage_fc)\n",
    "        tf.summary.histogram(\"adv\", self.advantage)\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "        self.merge_opt = tf.summary.merge_all()\n",
    "\n",
    "        return self.merge_opt\n",
    "\n",
    "    def get_graph(self):\n",
    "        return tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "DQNetwork = DDDQNNet(state_size, action_size, initial_learning_rate, name=\"DQNetwork\")\n",
    "\n",
    "# Instantiate the target network\n",
    "TargetNetwork = DDDQNNet(state_size, action_size, initial_learning_rate, name=\"TargetNetwork\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                 size=batch_size,\n",
    "                                 replace=False)\n",
    "\n",
    "        return [self.buffer[i] for i in index]\n",
    "\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "\n",
    "    for i in range(pretrain_length):\n",
    "        # If it's the first step\n",
    "        if i == 0:\n",
    "            state = env.reset(0)\n",
    "            state = np.array(state)\n",
    "\n",
    "        # Get the next_state, the rewards, done by taking a random action\n",
    "        choice = random.randint(1, len(possible_actions)) - 1\n",
    "        action = possible_actions[choice]\n",
    "\n",
    "        next_state, reward, done, _ = env.step(np.argmax(action))\n",
    "        next_state = np.array(next_state)\n",
    "#         next_state = next_state.reshape(next_state.shape + (1,))\n",
    "        # If the episode is finished (we're dead 3x)\n",
    "        if i == batch_size:\n",
    "            # We finished the episode\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            # Add experience to memory\n",
    "            memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "            # Our new state is now the next_state\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('./tensorboard', DQNetwork.get_graph())\n",
    "\n",
    "## Losses\n",
    "\n",
    "write_op = DQNetwork.record_tensorboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "\n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        # Make a random action (exploration)\n",
    "        choice = random.randint(1, len(possible_actions)) - 1\n",
    "        action = possible_actions[choice]\n",
    "\n",
    "    else:\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict={DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "\n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[choice]\n",
    "\n",
    "    return action, explore_probability\n",
    "\n",
    "def update_target_graph():\n",
    "    # Get the parameters of our DQNNetwork\n",
    "    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"DQNetwork\")\n",
    "\n",
    "    # Get the parameters of our Target_network\n",
    "    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"TargetNetwork\")\n",
    "\n",
    "    op_holder = []\n",
    "\n",
    "    # Update our target_network parameters with DQNNetwork parameters\n",
    "    for from_var, to_var in zip(from_vars, to_vars):\n",
    "        op_holder.append(to_var.assign(from_var))\n",
    "\n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.11883132,  0.06066014, -0.00394428, ...,  0.1375714 ,\n",
       "          0.12715119, -0.07063658],\n",
       "        [ 0.07949214,  0.06188746, -0.05324952, ..., -0.11989046,\n",
       "          0.01072435,  0.01268777],\n",
       "        [-0.04063517, -0.1341349 ,  0.00403014, ...,  0.1060271 ,\n",
       "         -0.0701232 ,  0.00863339],\n",
       "        ...,\n",
       "        [ 0.14630112,  0.02260889, -0.01164889, ..., -0.0430179 ,\n",
       "          0.14517242,  0.08155568],\n",
       "        [ 0.03963478, -0.13632327, -0.03896789, ...,  0.0862574 ,\n",
       "         -0.07230057, -0.05487594],\n",
       "        [-0.03553379, -0.00228639, -0.05219409, ..., -0.04660819,\n",
       "          0.0827335 ,  0.05833009]], dtype=float32),\n",
       " array([[-1.95397019e-01, -4.87519205e-02, -1.62187107e-02, ...,\n",
       "          3.71812433e-02,  5.99706061e-02, -3.73546928e-02],\n",
       "        [ 1.11205988e-02, -1.27366185e-01, -1.19850099e-01, ...,\n",
       "          6.49473583e-03,  4.64122295e-02, -5.39435372e-02],\n",
       "        [-1.60668828e-02, -2.80317292e-02, -1.08829021e-01, ...,\n",
       "          3.03863734e-02,  4.83046509e-02,  1.53517602e-02],\n",
       "        ...,\n",
       "        [-6.50283545e-02, -2.93073840e-02, -2.99125910e-02, ...,\n",
       "         -4.84005511e-02, -5.76691367e-02,  1.16996787e-01],\n",
       "        [ 1.19929034e-02,  1.84727293e-02,  1.05580548e-04, ...,\n",
       "         -5.67511693e-02, -5.70063666e-02,  3.61063555e-02],\n",
       "        [ 1.05980344e-01,  2.89176106e-02, -7.11006150e-02, ...,\n",
       "         -7.52911195e-02,  2.15262361e-02, -8.97423699e-02]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.], dtype=float32),\n",
       " array([[ 1.3209164e-02,  5.9461907e-02, -6.9070846e-02, ...,\n",
       "         -5.7699792e-02,  4.9753129e-02, -7.3706783e-02],\n",
       "        [ 1.5715848e-01, -2.3555756e-04,  1.6748790e-01, ...,\n",
       "          1.2972544e-01, -1.0666149e-01, -1.6794495e-01],\n",
       "        [-1.7855212e-02,  8.9306429e-02, -1.2107757e-01, ...,\n",
       "          1.7147769e-01, -1.1100799e-02,  3.7017792e-02],\n",
       "        ...,\n",
       "        [-7.0182212e-02, -3.0911893e-02, -1.6903815e-01, ...,\n",
       "          2.0110875e-02,  1.9435212e-02, -3.5072863e-03],\n",
       "        [ 1.5496279e-01,  1.3020784e-03, -1.1061406e-01, ...,\n",
       "          1.0491295e-01, -9.9857219e-02, -3.4527928e-03],\n",
       "        [-9.4438866e-02, -1.3632010e-01, -1.0064244e-04, ...,\n",
       "         -1.2767966e-01, -1.9537389e-02, -1.4922103e-01]], dtype=float32),\n",
       " array([[ 0.11481464, -0.10684663,  0.06309129, ..., -0.11496212,\n",
       "         -0.02387119,  0.0370008 ],\n",
       "        [-0.08265935, -0.0481192 , -0.01835606, ...,  0.02214937,\n",
       "          0.00601533, -0.02602789],\n",
       "        [-0.14486755, -0.15893744,  0.04425433, ..., -0.03502275,\n",
       "          0.01566745,  0.05653396],\n",
       "        ...,\n",
       "        [ 0.18650167, -0.12733719, -0.03491631, ..., -0.00770759,\n",
       "         -0.10539248,  0.12114418],\n",
       "        [ 0.06621066, -0.22207074,  0.16628817, ...,  0.0632489 ,\n",
       "          0.17200573,  0.16943845],\n",
       "        [-0.06913467,  0.09436493,  0.1910075 , ...,  0.00142417,\n",
       "         -0.0559619 ,  0.03386954]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 0.41433144],\n",
       "        [-0.19214682],\n",
       "        [-0.2232008 ],\n",
       "        [-0.0015834 ],\n",
       "        [ 0.09315687],\n",
       "        [-0.06995395],\n",
       "        [ 0.12898248],\n",
       "        [-0.39410758],\n",
       "        [-0.4152911 ],\n",
       "        [-0.19227633],\n",
       "        [ 0.27564287],\n",
       "        [ 0.40021074],\n",
       "        [ 0.38489467],\n",
       "        [ 0.04138294],\n",
       "        [ 0.3334992 ],\n",
       "        [ 0.280043  ],\n",
       "        [-0.38848162],\n",
       "        [ 0.29211324],\n",
       "        [ 0.38193142],\n",
       "        [-0.06992608],\n",
       "        [-0.3540867 ],\n",
       "        [ 0.10234171],\n",
       "        [ 0.24631226],\n",
       "        [ 0.30496746],\n",
       "        [ 0.21085292],\n",
       "        [-0.31145412],\n",
       "        [ 0.03082666],\n",
       "        [ 0.0582194 ],\n",
       "        [-0.1619724 ],\n",
       "        [-0.24765602],\n",
       "        [-0.02010447],\n",
       "        [-0.12760302]], dtype=float32),\n",
       " array([0.], dtype=float32),\n",
       " array([[-0.11419174,  0.11573614, -0.01701316, ...,  0.17650793,\n",
       "         -0.07320473,  0.07047345],\n",
       "        [-0.01969679,  0.17532457,  0.0931053 , ...,  0.11514898,\n",
       "         -0.16240129,  0.12342174],\n",
       "        [-0.09482518,  0.14483254,  0.14661999, ..., -0.08157387,\n",
       "          0.15903531, -0.07324132],\n",
       "        ...,\n",
       "        [-0.04071727, -0.1170665 ,  0.14801644, ...,  0.16237907,\n",
       "         -0.07142605,  0.00035079],\n",
       "        [-0.03515056,  0.09854122, -0.05979356, ...,  0.07753472,\n",
       "          0.0544257 , -0.10068667],\n",
       "        [ 0.14486967,  0.05044678,  0.08594273, ..., -0.14684737,\n",
       "          0.15663649,  0.12264927]], dtype=float32),\n",
       " array([[ 0.01955914,  0.00929323,  0.10897565, ...,  0.09545626,\n",
       "         -0.07821255, -0.02333337],\n",
       "        [-0.1015138 ,  0.08486725, -0.08169038, ...,  0.02138203,\n",
       "          0.06232941, -0.00646584],\n",
       "        [-0.06464712, -0.04720023, -0.12015542, ...,  0.0982381 ,\n",
       "         -0.02084109, -0.16286947],\n",
       "        ...,\n",
       "        [-0.16019975,  0.07074834, -0.02884663, ...,  0.01903395,\n",
       "         -0.1082928 ,  0.08691886],\n",
       "        [-0.09073655, -0.0248557 , -0.00639604, ..., -0.1134642 ,\n",
       "          0.14869916, -0.02122872],\n",
       "        [ 0.09881911,  0.01419102,  0.03251328, ..., -0.03778042,\n",
       "          0.00640272,  0.06188028]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 0.11537167, -0.15784448,  0.25505194,  0.36859688,  0.25368258,\n",
       "          0.14316145],\n",
       "        [-0.395138  , -0.08107501,  0.241905  ,  0.21662149, -0.10059455,\n",
       "          0.16630986],\n",
       "        [ 0.3622407 ,  0.17316219, -0.25784245,  0.3268992 ,  0.36599115,\n",
       "         -0.23792778],\n",
       "        [-0.34709498,  0.10018528,  0.02246284,  0.04657233,  0.01600266,\n",
       "          0.04865164],\n",
       "        [ 0.26536086,  0.03172678,  0.14350429,  0.18306866, -0.3610562 ,\n",
       "         -0.2826422 ],\n",
       "        [-0.2212666 , -0.38792846,  0.14300296, -0.14282009,  0.38603798,\n",
       "          0.38814613],\n",
       "        [ 0.04534897,  0.18758705,  0.17267475,  0.37735882,  0.3225117 ,\n",
       "          0.00112927],\n",
       "        [ 0.21463153, -0.3936434 , -0.16848725, -0.18614413,  0.00622115,\n",
       "         -0.104072  ],\n",
       "        [-0.27804685, -0.19551134,  0.19064543,  0.0776279 ,  0.3211235 ,\n",
       "         -0.19494319],\n",
       "        [-0.02388778,  0.27614495, -0.3285001 ,  0.3665606 , -0.31709433,\n",
       "          0.14142242],\n",
       "        [ 0.03346816,  0.34994414,  0.03687021,  0.11178491, -0.29024982,\n",
       "         -0.03413352],\n",
       "        [-0.19745982,  0.39629057, -0.29797024,  0.03668073,  0.05403805,\n",
       "          0.04609218],\n",
       "        [ 0.30758724,  0.10800335, -0.10161772,  0.14480236, -0.3336938 ,\n",
       "          0.23981348],\n",
       "        [-0.3316525 ,  0.33884653,  0.17285863, -0.24781738, -0.33550918,\n",
       "          0.24611089],\n",
       "        [ 0.16474769, -0.1620066 , -0.12320384,  0.26161274,  0.16475013,\n",
       "         -0.29127377],\n",
       "        [ 0.14371088,  0.30807278, -0.3265033 , -0.17034355,  0.03412187,\n",
       "          0.10713944],\n",
       "        [ 0.15971377, -0.24524488,  0.13618258,  0.04326841, -0.02325815,\n",
       "          0.01846793],\n",
       "        [-0.01116562,  0.29379258,  0.3074458 , -0.26849914, -0.13169274,\n",
       "         -0.14481279],\n",
       "        [-0.02867612, -0.24083264, -0.29393002, -0.19507763, -0.23347671,\n",
       "         -0.35276145],\n",
       "        [ 0.3597714 , -0.07030776,  0.31267944, -0.03360972,  0.13975218,\n",
       "          0.39059123],\n",
       "        [ 0.23403916,  0.37684467, -0.04583138, -0.27160826,  0.38283136,\n",
       "          0.23241857],\n",
       "        [-0.15356508,  0.39080736,  0.19885948, -0.16584179, -0.1394409 ,\n",
       "          0.3401309 ],\n",
       "        [ 0.22992954, -0.34519756, -0.33251253,  0.37943992, -0.22145087,\n",
       "         -0.30771685],\n",
       "        [ 0.3019142 , -0.2268911 , -0.31615812,  0.00354603,  0.12926582,\n",
       "          0.22095606],\n",
       "        [ 0.02539885, -0.36666754,  0.01317036,  0.21865973, -0.1456163 ,\n",
       "         -0.20468548],\n",
       "        [ 0.15669277,  0.22507486,  0.21200863,  0.24633595,  0.31789353,\n",
       "         -0.16339149],\n",
       "        [-0.08429715,  0.10620376,  0.12963012, -0.03524953, -0.25821024,\n",
       "         -0.35564545],\n",
       "        [-0.18934438, -0.00070807, -0.08527341, -0.05040181,  0.29964414,\n",
       "          0.14876375],\n",
       "        [-0.38329557,  0.2865691 ,  0.16381696,  0.29845688, -0.21548247,\n",
       "          0.02184078],\n",
       "        [ 0.20555642,  0.12364098, -0.09506288, -0.08566752,  0.24513647,\n",
       "         -0.29592   ],\n",
       "        [-0.3742896 ,  0.24585244,  0.17604259,  0.24140629,  0.03444123,\n",
       "         -0.09339038],\n",
       "        [ 0.26889125,  0.26619825,  0.10641047, -0.16777822,  0.2269347 ,\n",
       "         -0.19208533]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver(max_to_keep=total_loop)\n",
    "\n",
    "total_step = 0\n",
    "decay_step = 0\n",
    "tau = 0\n",
    "\n",
    "rewards_list = []\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "update_target = update_target_graph()\n",
    "sess.run(update_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop = 0,average total reward = -0.2567697340067221,Training loss = 0.20574800670146942,Explore P = 0.9399715412348315\n",
      "Loop = 1,average total reward = -0.13253901138365562,Training loss = 0.29685693979263306,Explore P = 0.8851321419868909\n",
      "Loop = 2,average total reward = -0.11569940588747112,Training loss = 0.2405315786600113,Explore P = 0.8249125548251013\n",
      "Loop = 3,average total reward = 0.012786716049393743,Training loss = 0.08115313202142715,Explore P = 0.7762904157642412\n",
      "Loop = 4,average total reward = -0.26095956742021686,Training loss = 0.15550589561462402,Explore P = 0.7346496794588482\n",
      "Loop = 5,average total reward = -0.1042947806637644,Training loss = 0.13304075598716736,Explore P = 0.6905914615951421\n",
      "Model updated\n",
      "Loop = 6,average total reward = 0.2498649101660975,Training loss = 0.3348473608493805,Explore P = 0.6399282861772813\n",
      "Loop = 7,average total reward = 0.14009536956871368,Training loss = 0.28191685676574707,Explore P = 0.6004591355293021\n",
      "Loop = 8,average total reward = -0.14385703703702846,Training loss = 0.9706680774688721,Explore P = 0.5689317721669267\n",
      "Loop = 9,average total reward = 0.304411613906572,Training loss = 0.3955063819885254,Explore P = 0.5285246022018764\n",
      "Model updated\n",
      "Loop = 10,average total reward = 0.365312689226559,Training loss = 0.18311315774917603,Explore P = 0.48934267943577864\n",
      "Loop = 11,average total reward = 0.11899381265032921,Training loss = 0.45453619956970215,Explore P = 0.4610238356984316\n",
      "Loop = 12,average total reward = 0.07743164407815613,Training loss = 0.14479179680347443,Explore P = 0.43430219824903593\n",
      "Loop = 13,average total reward = 0.06463456373148688,Training loss = 0.7623598575592041,Explore P = 0.4126423672711413\n",
      "Loop = 14,average total reward = 0.20981313002901997,Training loss = 0.2710771858692169,Explore P = 0.3899216706968521\n",
      "Model updated\n",
      "Loop = 15,average total reward = 0.22457813932981655,Training loss = 1.195900797843933,Explore P = 0.3681088569606118\n",
      "Loop = 16,average total reward = 0.2541247516187597,Training loss = 0.46806997060775757,Explore P = 0.34984654358556566\n",
      "Loop = 17,average total reward = 0.3774775908808035,Training loss = 0.5662382245063782,Explore P = 0.3302703267121576\n",
      "Loop = 18,average total reward = 0.10628062440647056,Training loss = 1.0235843658447266,Explore P = 0.3160024982680432\n",
      "Model updated\n",
      "Loop = 19,average total reward = 0.2844327244043547,Training loss = 0.5229513645172119,Explore P = 0.30216730633005906\n",
      "Loop = 20,average total reward = 0.24574676366843928,Training loss = 0.8778150677680969,Explore P = 0.2920757610340308\n",
      "Loop = 21,average total reward = 0.2234263991462558,Training loss = 0.38866904377937317,Explore P = 0.2822375267482291\n",
      "Loop = 22,average total reward = 0.32663740740741465,Training loss = 0.4598964750766754,Explore P = 0.2728652414518585\n",
      "Model updated\n",
      "Loop = 23,average total reward = 0.08048807720545909,Training loss = 0.8889536261558533,Explore P = 0.26501459139271333\n",
      "Loop = 24,average total reward = 0.2863196402116509,Training loss = 0.6697177886962891,Explore P = 0.25871009731729716\n",
      "Loop = 25,average total reward = 0.1838414480944016,Training loss = 0.19025570154190063,Explore P = 0.2531762425324948\n",
      "Loop = 26,average total reward = 0.30097401947435976,Training loss = 0.7867941856384277,Explore P = 0.2471630962498978\n",
      "Loop = 27,average total reward = 0.2863393518518555,Training loss = 0.49561986327171326,Explore P = 0.242334896727013\n",
      "Model updated\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1dc96b94576e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    102\u001b[0m                                    feed_dict={DQNetwork.inputs_: states_mb,\n\u001b[1;32m    103\u001b[0m                                               \u001b[0mDQNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_Q\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargets_mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                                               DQNetwork.actions_: actions_mb})\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtau\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_tau\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for loop in range(total_loop):\n",
    "    total_reward_list = []\n",
    "\n",
    "    total_epi = np.arange(total_episodes)\n",
    "    np.random.shuffle(total_epi)\n",
    "\n",
    "    for episode in total_epi:\n",
    "\n",
    "        # Set step to 0\n",
    "        step = 0\n",
    "\n",
    "        # Initialize the rewards of the episode\n",
    "        episode_rewards = []\n",
    "\n",
    "        # Make a new episode and observe the first state\n",
    "        state = env.reset(num_days = 0)\n",
    "        state = np.array(state)\n",
    "#         state = state.reshape(state.shape + (1,))\n",
    " \n",
    "        while step < max_steps:\n",
    "            step += 1\n",
    "\n",
    "            # Increase the C step\n",
    "            tau += 1\n",
    "\n",
    "            # Increase decay_step\n",
    "            decay_step += 1\n",
    "\n",
    "            # With ϵ select a random action atat, otherwise select a = argmaxQ(st,a)\n",
    "            action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state,\n",
    "                                                             possible_actions)\n",
    "\n",
    "            # Do the action\n",
    "            next_state, reward, done, _ = env.step(np.argmax(action))\n",
    "            next_state = np.array(next_state)\n",
    "#             next_state = next_state.reshape(next_state.shape + (1,))\n",
    "\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            # If the game is finished\n",
    "            if done:\n",
    "                # Set step = max_steps to end the episode\n",
    "                step = max_steps\n",
    "                memory.add((state, action, reward, next_state, done))\n",
    "                total_reward = np.mean(episode_rewards)\n",
    "                total_reward_list.append(total_reward)\n",
    "                \n",
    "#                 print(f'Loop = {loop},'\n",
    "#                         f'Episode = {episode},'\n",
    "#                         f'total reward = {total_reward},'\n",
    "#                         f'Training loss = {loss},'\n",
    "#                         f'Explore P = {explore_probability}')\n",
    "\n",
    "                rewards_list.append((loop, episode, total_reward))\n",
    "\n",
    "            else:\n",
    "                memory.add((state, action, reward, next_state, done))\n",
    "                state = next_state\n",
    "\n",
    "            ### LEARNING PART\n",
    "            # Obtain random mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "            actions_mb = np.array([each[1] for each in batch])\n",
    "            rewards_mb = np.array([each[2] for each in batch])\n",
    "            next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "            dones_mb = np.array([each[4] for each in batch])\n",
    "\n",
    "            target_Qs_batch = []\n",
    "\n",
    "            ### DOUBLE DQN Logic\n",
    "            # Use DQNNetwork to select the action to take at next_state (a') (action with the highest Q-value)\n",
    "            # Use TargetNetwork to calculate the Q_val of Q(s',a')\n",
    "\n",
    "            # Get Q values for next_state\n",
    "            q_next_state = sess.run(DQNetwork.output, feed_dict={DQNetwork.inputs_: next_states_mb})\n",
    "\n",
    "            # Calculate Qtarget for all actions that state\n",
    "            q_target_next_state = sess.run(TargetNetwork.output, feed_dict={TargetNetwork.inputs_: next_states_mb})\n",
    "\n",
    "            # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma * Qtarget(s',a')\n",
    "            for i in range(0, len(batch)):\n",
    "                terminal = dones_mb[i]\n",
    "\n",
    "                # We got a'\n",
    "                action = np.argmax(q_next_state[i])\n",
    "\n",
    "                # If we are in a terminal state, only equals reward\n",
    "                if terminal:\n",
    "                    target_Qs_batch.append(rewards_mb[i])\n",
    "\n",
    "                else:\n",
    "                    # Take the Qtarget for action a'\n",
    "                    target = rewards_mb[i] + gamma * q_target_next_state[i][action]\n",
    "                    target_Qs_batch.append(target)\n",
    "\n",
    "            targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                #                     _, rate = sess.run([DQNetwork.add_global, DQNetwork.learning_rate])\n",
    "\n",
    "            _, loss = sess.run([DQNetwork.optimizer, DQNetwork.loss],\n",
    "                                   feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                              DQNetwork.target_Q: targets_mb,\n",
    "                                              DQNetwork.actions_: actions_mb})\n",
    "\n",
    "            if tau > max_tau:\n",
    "                # Update the parameters of our TargetNetwork with DQN_weights\n",
    "                update_target = update_target_graph()\n",
    "                sess.run(update_target)\n",
    "                tau = 0\n",
    "                print(\"Model updated\")\n",
    "                \n",
    "                \n",
    "            # Write TF Summaries\n",
    "            summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\n",
    "                                                    DQNetwork.target_Q: targets_mb,\n",
    "                                                    DQNetwork.actions_: actions_mb})\n",
    "\n",
    "            writer.add_summary(summary, total_step)\n",
    "            total_step += 1\n",
    "            \n",
    "\n",
    "    print(f'Loop = {loop},'\n",
    "        f'average total reward = {np.mean(total_reward_list)},'\n",
    "        f'Training loss = {loss},'\n",
    "        f'Explore P = {explore_probability}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_performance(which_day):\n",
    "    \n",
    "    state = env.reset(which_day)\n",
    "    state = np.array(state)\n",
    "    all_reward = []\n",
    "    \n",
    "    for step in range(5000):\n",
    "\n",
    "        Qs = sess.run(DQNetwork.output_softmax, feed_dict={DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[int(choice)]\n",
    "        next_state, reward, done, _ = env.step(np.argmax(action))\n",
    "        all_reward.append(reward)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        else:\n",
    "            # If not done, the next_state become the current state\n",
    "            next_state = np.array(next_state)\n",
    "            state = next_state\n",
    "\n",
    "    return np.sum(all_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.973200000000066\n"
     ]
    }
   ],
   "source": [
    "check_reward = test_performance(which_day=0)\n",
    "print(check_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test]",
   "language": "python",
   "name": "conda-env-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
